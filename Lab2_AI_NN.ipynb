{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travaux Pratiques Intelligence Artificielle: Lab2\n",
    "\n",
    "Neural Networks avec Pytorch ü§ñ üôå.\n",
    "\n",
    "## Objectifs du TP:\n",
    "\n",
    "> **Se Familiariser avec Pytorch** \n",
    "\n",
    "> **Comprendre le processus d'apprentissage d'un r√©seau de neurones**\n",
    "\n",
    "> **Cr√©er un r√©seau de neurones (feed-forward multilayers NN) avec Pytorch**\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "_Besoin d'aide? Laisser moi un Commentaire sur Teams_\n",
    "\n",
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Introduction √† Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Pytorch est une librairie open source Python, developp√©e par AI Research lab (FAIR) de Facebook et publi√©e en ligne en Sept. 2016. Elle permet d'effectuer les calculs tensoriels n√©cessaires notamment pour l'apprentissage profond (deep learning).   \n",
    "\n",
    "Pytorch permet une impl√©mentation et un apprentissage facile des r√©seaux de neurones (NN). Elle est tr√®s proche de la syntaxe de Python, d'ailleurs il est m√™me possible d'int√©grer des blocs if else dans un code NN. Elle a une riche communaut√© et une documentation actualis√©e (compar√©e √† tensorflow). D'autant plus qu'elle est tr√®s utilis√©e par les chercheurs, ce qui garantie l'impl√©mentation des nouveaux mod√®les sur Pytorch.\n",
    "\n",
    "Vous pouvez installer Pytorch √† partir d'un terminal ou au niveau de ce jupyter en ex√©cutant la cellule ci-dessous. \n",
    "\n",
    "**N.B.** Si vous avez des inconsistences au niveau de l'installation, pensez √† cr√©er un environnement sp√©cifique pour ce TP.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\nSolving environment: ...working... done\n\n# All requested packages already installed.\n\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Afin de mener √† bien les impl√©mentations des NN, Pytorch fournit l'objet **Tensor**. Un tensor sur Pytorch a les m√™mes fonctions et syntaxe qu'un numpy array mais est optimis√© pour GPU. D'autant plus que son backend C++ permet √† Pytorch d'√™tre beaucoup plus performante et rapide que numpy. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr√©er une matrice identit√© de taille 4x4\n",
    "T = [[0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0.],\n",
    "     [0., 0., 0., 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4.7399, 6.0589, 5.5831, 0.9880, 4.2675, 5.0078, 1.4938, 0.3686, 7.8500,\n",
       "         9.9098, 4.7377, 8.6780, 6.3938, 2.9011, 4.7218, 3.0497]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Cr√©er un tensor avec des valeurs al√©atoires d'une taille de votre choix\n",
    "tsr = 10*torch.rand((1,16))\n",
    "tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4.7399, 6.0589, 5.5831, 0.9880],\n",
       "        [4.2675, 5.0078, 1.4938, 0.3686],\n",
       "        [7.8500, 9.9098, 4.7377, 8.6780],\n",
       "        [6.3938, 2.9011, 4.7218, 3.0497]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Changer la forme de votre tensor \n",
    "tst = tsr.view(4,4)\n",
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 4])\ntorch.float32\n"
     ]
    }
   ],
   "source": [
    "# Afficher la taille, la forme, et le type des √©l√©ments de votre tensor\n",
    "print(tst.size())  #la forme\n",
    "print(tst.dtype)   #le type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposer le tensor \n",
    "trans = torch.transpose(tst,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[14.7399, 16.0589, 15.5831, 10.9880],\n",
       "        [14.2675, 15.0078, 11.4938, 10.3686],\n",
       "        [17.8500, 19.9098, 14.7377, 18.6780],\n",
       "        [16.3938, 12.9011, 14.7218, 13.0497]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Ajouter une constante fixe √† l'ensemble des √©l√©ments du tensor\n",
    "torch.add(tst , 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 91.3236,  59.2734, 132.2752,  77.2587],\n",
       "        [ 59.2734,  45.6568,  93.4024,  49.9914],\n",
       "        [132.2752,  93.4024, 257.5803, 127.7768],\n",
       "        [ 77.2587,  49.9914, 127.7768,  80.8930]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Multiplier votre tensor par sa transpos√© (multiplication matricielle, pas element-wise)\n",
    "torch.matmul(tst,trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(4.7218)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Avec quelques exemples, montrer que la selection d'√©l√©ments des tensors se fait de la m√™me mani√®re que sur NumPy \n",
    "tst[3][2] # c'est exactement la valeur en (4eme ligne et 3eme colonne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "Une fonctionnalit√© tr√®s utile de Pytorch, est la possibilit√© d'√©changer les tensors facilement avec NumPy.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer numpy\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5, 4, 4, 3, 5, 3, 3, 4, 1, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Cr√©er une matrice numpy avec des valeurs al√©atoires\n",
    "T2 = np.random.random_integers(1,5,10)\n",
    "T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([5, 4, 4, 3, 5, 3, 3, 4, 1, 3], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Cr√©er un tensor Pytorch √† base de cette matrice np\n",
    "tsr2 = torch.tensor(T2)\n",
    "tsr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5, 4, 4, 3, 5, 3, 3, 4, 1, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Convertir √† nouveau ce tensor en un numpy array\n",
    "T3 = tsr2.numpy()\n",
    "T3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Pytorch, comme numpy, offre une fonctionnalit√© qu'on appelle **broadcasting**. Elle permet d'ex√©cuter des op√©rations entre des tensors (resp. arrays) qui n'ont pas la m√™me forme, comme l'illustre la figure suivante:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/broadcasting.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch assouplie la contrainte de forme mais ne la supprime pas totalement. Il est primordiale d'ob√©ir les r√®gles pour effectuer des opr√©rations entre deux tensors Pytorch:\n",
    "1. Deux tensors de m√™me rang sont compatibles (broadcastable) si, pour chaque axe, soit les tailles sont √©gales, soit l‚Äôune d‚Äôelles est exactement √©gale √† 1. P.ex. (5, 3) et (1, 3) sont des formats broadcastable, (5, 3) et (5, 1) √©galement, mais (5, 3) et (3, 1) ne le sont pas.\n",
    "2. Si un tensor a un axe de taille 1, le tensor sera dupliqu√© √† la vol√©e autant de fois que n√©cessaire selon cet axe pour atteindre la taille de l‚Äôautre tensor le long de cet axe. P.ex. un tensor (2, 1, 3) pourra √™tre transform√© en tensor (2, 5, 3) en le dupliquant 5 fois le long du 2e axe (axis=1).\n",
    "3. La taille selon chaque axe apr√®s broadcast est √©gale au maximum de toutes les tailles d‚Äôentr√©e le long de cet axe. P.ex. (5, 3, 1) √ó (1, 3, 4) ‚Üí (5, 3, 4).\n",
    "4. Si un des tensors a un rang (ndim) inf√©rieur √† l‚Äôautre, alors sa forme (shape) est pr√©c√©d√©e d‚Äôautant de 1 que n√©cessaire pour atteindre le m√™me rang. P.ex. (5, 3, 1) √ó (4,) = (5, 3, 1) √ó (1, 1, 4) ‚Üí (5, 3, 4).\n",
    "\n",
    "Pour plus d'informations sur le broadcasting, vous pouvez aller sur le lien: http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[19, 14, 19, 15, 12],\n",
       "       [19, 14, 19, 15, 12],\n",
       "       [13,  8, 13,  9,  6],\n",
       "       [12,  7, 12,  8,  5],\n",
       "       [12,  7, 12,  8,  5]])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Illustrer le broadcasting par un exemple\n",
    "A = np.random.random_integers(1,10,5)\n",
    "B = np.random.random_integers(1,10,5)\n",
    "tab = np.array([ B for i in range(len(B))])\n",
    "A = A.reshape(5,1)                   # A(5,1) ; B(5,1)-> tab(5,5) // A(5,1) - tab(5,5)   \n",
    "tab + A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> L'unes des fonctionnalit√©s les plus pris√©e de Pytorch est qu'il permet de calculer d'une mani√®re tr√®s facile les gradients des tensors, ce qui est tr√®s utile pour appliquer les algorithmes d'optimisation par descente de gradient. Pour cela, PyTorch utilise la biblioth√®que autograd qui permet de garder une trace de l'orgine des tensors (leurs tensors parents, les op√©rations effectu√©es) et fournir automatiquement la cha√Æne des d√©riv√©es de ces op√©rations en fonction de leur input. Pytorch fournit donc le gradient d'une expression en fonction de ses param√®tres en input automatiquement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un tensor x dont la fonctionalit√© de gradient est activ√©e\n",
    "#x = torch.autograd.Variable(torch.Tensor([2]),requires_grad=True)\n",
    "x = torch.autograd.Variable(torch.Tensor([2.,10.,5.,6.,8.]),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un nouveau tensor y, r√©sultat d'une op√©ration lin√©aire sur le tensor x. L'attribut grad_fn trace l'op√©ration effectu√©e \n",
    "y = x.sin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un tensor w qui est la moyenne de y. Vous devrez voir la diff√©rence dans grad_fn\n",
    "w = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire un backpropagation sur votre tensor w -> ce qui revient √† automatiquement calculer dw/dx\n",
    "w.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.0832, -0.1678,  0.0567,  0.1920, -0.0291])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Afficher le gradient de x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([3.6269e+00, 1.1013e+04, 7.4203e+01, 2.0171e+02, 1.4905e+03],\n",
       "       grad_fn=<SinhBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# Effectuer une op√©ration sur x qui n'est pas trac√© dans le gradient  \n",
    "#y.backward(torch.tensor([1.,1.,1.,1.,1.]))\n",
    "x.sinh()                #On essaie d'appliquer une nouvelle fonction √† x, mais sans l'application du backward les valeurs du                                             gradient ne changent pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.0832, -0.1678,  0.0567,  0.1920, -0.0291])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Afficher le gradient de x (devrait rester le m√™me)\n",
    "x.grad          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Pytorch offre √©galement la possibilit√© d'effectuer les op√©rations des tensors sur CPU ou GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# V√©rifier que vous avez la capacit√© d'ex√©cuter sur GPU sur votre machine\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si vous avez GPU, vous pouvez d√©placer les calculs qu'on vient de faire sur GPU\n",
    "cuda = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Vous pouvez explorer d'autres op√©rations torch peuvent √™tre explor√©es ici: https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Notre premier NN\n",
    "\n",
    "![image](resources/learning.png)\n",
    "\n",
    "Source: Deep Learning with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Dans notre TP, nous allons impl√©menter un feed forward NN qui identifie les chiffres √©crits √† la main (Hello World! des NN). La difficult√© revient √† identifier le m√™me chiffre m√™me s'il est √©crit d'une mani√®re tr√®s diff√©rente. \n",
    "\n",
    "Notre NN devra prendre en input une image (28 x 28 pixels) et identifier le chiffre en output. Le Dataset utilis√©, __MNIST__, englobe 60.000 √©chantillions de chiffres √©crits √† la main par 250 personnes pour l'apprentissage, et 10.000 pour le test √©crits par un autre groupe de 250 personnes. Les √©chantillons ont √©t√© rassembl√©s et merg√©s par NIST (United States' National Institute of Standards and Technology), d'o√π le nom du dataset. \n",
    "\n",
    "Pour ce faire, nous allons cr√©er un r√©seau de neurones qui permet de **classfier** une image selon un **output label**. \n",
    "\n",
    "- Combien de Labels faudrait-il pr√©voir en output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les √©tapes pour la cr√©ation de notre NN sont les suivants:\n",
    "\n",
    "1. Lire et pr√©parer les donn√©es \n",
    "\n",
    "2. Cr√©er le r√©seau de neuronne\n",
    "\n",
    "3. Entra√Æner le r√©seau de neuronne\n",
    "\n",
    "4. Tester notre mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Lire et pr√©parer les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PO-N2fynM53t"
   },
   "source": [
    "Avant d'impl√©menter tout mod√®le, il faut commencer par **Examiner et comprendre les donn√©es** puis les **formatter** convenablement pour alimenter le NN. L'objectif de cette partie est d'abord d'explorer le dataset MNIST, puis de pr√©parer les datasets input du NN.\n",
    "\n",
    "\n",
    "1.   Importer le MNIST dataset\n",
    "2.   Afficher les images avec leurs labels\n",
    "3.   Pr√©parer les donn√©es: split train, test, validation et normalisation des donn√©es\n",
    "4.   Cr√©er un Pytorch Dataset\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.   Importer le MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer la librairie torch et le module datasets de torchvision\n",
    "from torchvision import datasets , transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Le projet torchvision contient les meilleures architectures NN pour computer vision, ainsi que des datasets standards et utilitaires pour impl√©menter les projest computer vision. La liste des datasets du projet peut √™tre explor√©e ici: https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "\n",
    "Le dataset MNIST contient un sous dataset pour le training et un pour le test. R√©cup√©rer ces deux sous-dataset en deux objets dataset https://pytorch.org/docs/stable/torchvision/datasets.html#mnist\n",
    "\n",
    "N.B. Ne pas oublier de donner des noms significatifs √† vos variables.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer les donn√©es d'apprentissage\n",
    "TrainSet = datasets.MNIST('PATH_TO_STORE_TRAINSET',train=True,download=True,transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©rer les donn√©es de test\n",
    "TestSet = datasets.MNIST('PATH_TO_STORE_TESTSET',train=True,download=True,transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Essayons d'explorer les donn√©es dans nos datasets. MNIST devrait contenir des images de chiffres √©crits √† la main. Une image est repr√©sent√© comme un Tensor Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher un datapoint de votre dataset d'apprentissage, sa forme, le type des donn√©es, les valeurs min et max. \n",
    "# Que repr√©sente les valeurs du Tensor √† votre avis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 1.2.  Afficher les images avec leurs labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utiliserons imshow de matplotlib pour afficher un √©chantillon des donn√©es. Pensez √† √©galement afficher le label correspondant √† chaque image en utilisant l'attribut targets votre objet MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fce3275280>"
      ]
     },
     "metadata": {},
     "execution_count": 32
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-08T17:29:35.066496</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p1417dc98eb)\">\r\n    <image height=\"218\" id=\"imagef16fc983bc\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mf67da059b4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mf67da059b4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mf67da059b4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mf67da059b4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mf67da059b4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mf67da059b4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mf67da059b4\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m00de65c769\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m00de65c769\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m00de65c769\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m00de65c769\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m00de65c769\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m00de65c769\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m00de65c769\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p1417dc98eb\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Afficher un √©chantillon des images du dataset d'apprentissage avec leurs labels respectifs \n",
    "plt.imshow(TrainSet[0][0].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 1.3.  Pr√©parer les donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Cr√©er un dataset de validation**\n",
    "\n",
    "Notre dataset contient les donn√©es d'apprentissage et de test, nous aurons √©galement besoin d'un dataset de validation. Pour ce faire, nous allons diviser notre dataset d'apprentissage de mani√®re √† d√©dier **10.000** data point √† la validation. \n",
    "\n",
    "Nous utiliserons la librairie scikit learn qui contient une fonction permettant de diviser les donn√©es d'une mani√®re tr√®s efficace. Pour plus de d√©tails, voir: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la fonction de splitting des donn√©es de scikit learn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er 4 tensors en r√©sultat du splitting: donn√©es d'apprentissage, donn√©es de validation, labels d'apprentissage, et labels de validation  \n",
    "app, val, lab_app, lab_val = train_test_split(TrainSet,TestSet, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.2588, 0.7686, 0.7216, 0.2549,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3216,\n",
       "            0.9882, 0.9882, 0.5882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4353, 0.9176,\n",
       "            0.9882, 0.6941, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3961, 0.9451, 0.9882,\n",
       "            0.8392, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0275, 0.4588, 0.9451, 0.9961, 0.9922,\n",
       "            0.9922, 0.2078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0275, 0.5882, 0.9882, 0.9882, 0.9922, 0.9882,\n",
       "            0.9882, 0.9882, 0.3961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.4588, 0.9882, 0.9882, 0.6941, 0.4157, 0.3922,\n",
       "            0.9882, 0.9882, 0.9686, 0.2588, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.1098, 0.5451, 0.4039, 0.0118, 0.0000, 0.0275,\n",
       "            0.7961, 0.9882, 0.9922, 0.7216, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n",
       "            0.6039, 0.9922, 0.9961, 0.5255, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235,\n",
       "            0.9882, 0.9882, 0.9176, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.6745,\n",
       "            0.9882, 0.9882, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5020, 0.9882,\n",
       "            0.9882, 0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0902, 0.7020, 0.9961, 0.9686,\n",
       "            0.3922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.1255, 0.3216, 0.9176, 0.9882, 0.9686, 0.4039,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.8980, 0.9882, 0.9882, 0.8392, 0.3922, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.9922, 0.8902, 0.4039, 0.0588, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       "  3),\n",
       " (tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0431, 0.9569, 0.2392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.3176, 0.9922, 0.1294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.4549, 0.9922, 0.9255, 0.1725, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.7216, 0.9882, 0.6196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.5725, 0.9922, 0.9882, 0.2706, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.7216, 0.9882, 0.9882, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.9020, 0.9922, 0.8392, 0.0510, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.7216, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686,\n",
       "            0.9412, 0.9922, 0.8078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.7216, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078,\n",
       "            0.9922, 1.0000, 0.8118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.7255, 0.9922, 0.9922, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8118,\n",
       "            0.9882, 0.9922, 0.8078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
       "            0.7725, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8118,\n",
       "            0.9882, 0.9922, 0.9412, 0.7216, 0.7216, 0.7216, 0.5176, 0.7725,\n",
       "            0.9882, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8118,\n",
       "            0.9882, 0.9922, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882,\n",
       "            0.9882, 0.9882, 0.9882, 0.8980, 0.4627, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8118,\n",
       "            0.9882, 0.9922, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882,\n",
       "            0.9882, 0.9882, 0.9882, 0.9922, 0.8078, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3725,\n",
       "            0.6588, 0.9059, 0.9020, 0.9020, 0.9020, 0.9020, 0.9059, 0.9608,\n",
       "            0.9922, 0.9922, 0.9922, 1.0000, 0.7686, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6314,\n",
       "            0.9882, 0.9882, 0.9882, 0.6471, 0.1294, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3059,\n",
       "            0.9882, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1843,\n",
       "            0.9882, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1843,\n",
       "            0.9882, 0.9882, 0.9882, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235,\n",
       "            0.9922, 0.9922, 0.9922, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6314,\n",
       "            0.9882, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3059,\n",
       "            0.9882, 0.9882, 0.9882, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1176,\n",
       "            0.8902, 0.9882, 0.9882, 0.2902, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.5137, 0.6157, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       "  4),\n",
       " (tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.1176, 0.3804, 0.6784, 0.9961,\n",
       "            0.7686, 0.5647, 0.2235, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.1608, 0.5804, 0.9569, 0.9961, 0.9922, 0.9922,\n",
       "            0.9922, 0.9922, 0.9922, 0.8314, 0.1020, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.1373, 0.7882, 0.9294, 0.9922, 0.9922, 0.9451, 0.6039, 0.2471,\n",
       "            0.4000, 0.6863, 0.9922, 0.9922, 0.5569, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.3725,\n",
       "            0.8745, 0.9922, 0.9922, 0.9294, 0.5255, 0.2275, 0.0000, 0.0000,\n",
       "            0.0000, 0.0118, 0.4118, 0.9922, 0.7804, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3725, 0.9922,\n",
       "            0.9922, 0.8706, 0.3451, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.1451, 0.9216, 0.9922, 0.3882, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.6471,\n",
       "            0.3098, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.3725, 0.9922, 0.9608, 0.0863, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392,\n",
       "            0.6863, 0.9647, 0.9804, 0.3412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.1765, 0.8549,\n",
       "            0.9922, 0.8627, 0.2549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.2667, 0.7725, 0.9922, 0.9922,\n",
       "            0.7961, 0.2039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.4588, 0.9765, 0.9961, 0.9922, 0.9922,\n",
       "            0.9294, 0.5843, 0.0941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.1333, 0.8118, 0.4392, 0.4353, 0.7922,\n",
       "            0.9373, 1.0000, 0.9961, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.1608, 0.6392, 0.9922, 0.9608, 0.1020, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.2196, 0.9922, 0.9922, 0.3882, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.2196, 0.9922, 0.9882, 0.3569, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.5020, 0.9922, 0.8549, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.4118, 0.9216, 0.9922, 0.1765, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0471, 0.5725,\n",
       "            0.9529, 0.9843, 0.4588, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0784, 0.4745, 0.9922, 0.9922,\n",
       "            0.6000, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4235, 0.8510, 0.7373,\n",
       "            0.7373, 0.7373, 0.7373, 0.7373, 0.8510, 0.9961, 0.9137, 0.3333,\n",
       "            0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6353, 0.9922, 0.9922,\n",
       "            0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.5059, 0.0941, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "            0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       "  3),\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# R√©cup√©rer les labels de test √† partir de votre sous-dataset de test\n",
    "app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Vous devez maintenant avoir 6 tensors, un pour les donn√©es et l'autre pour les labels et ce pour l'apprentissage, le test, et la validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# Afficher la taille de chaque tensor\n",
    "app[0][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "**Normaliser les donn√©es**\n",
    "\n",
    "Il est souvent recommand√© de normaliser les donn√©es avant d'alimenter le NN pour am√©liorer sa performance. On peut facilement redimensionner √† l'intervalle [0,1] les valeurs de nos donn√©es en utilisant la moyenne et la d√©viation standard suivant cette formule: $$(Data - Mean) / standard Deviation$$\n",
    "\n",
    "Comme \"thumb rule\", on normalise les donn√©es de test et de validation en utilisant la moyenne et la d√©viation des donn√©es d'apprentissage. Le raisonnement √©tant que les donn√©es test et validation sont indisponibles au moment de la cr√©ation du NN.   \n",
    "\n",
    "Apr√®s normalisation, le mean devrait √™tre 0 et standard deviation √† 1 (ou tr√®s proche resp. de 0 et 1).\n",
    "\n",
    "N.B. Il faudra convertir les valeurs de vos tensors en float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les valeurs des tensors en float\n",
    "#app[0][0].long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stocker dans des variables la moyenne et la d√©viation standard des donn√©es d'apprentissage \n",
    "def moyenne(lst):\n",
    "    tab = []\n",
    "    for i in range(len(lst)):\n",
    "        tab += [lst[i][0].numpy().mean()]\n",
    "    return torch.tensor(tab).mean()\n",
    "def deviation(lst):\n",
    "    tab = []\n",
    "    for i in range(len(lst)):\n",
    "        tab += [torch.std(lst[i][0])]\n",
    "    return torch.tensor(tab).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.1308)\n",
      "tensor(0.3016)\n"
     ]
    }
   ],
   "source": [
    "mean = moyenne(app)         # table des moyennes de chaque tensor\n",
    "print(mean)      \n",
    "dev = deviation(app)        # table des d√©viations de chaque tensor\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliser les donn√©es d'apprentissage, de test, et de validation par ces valeurs\n",
    "def normalize(data):\n",
    "    data1 = []\n",
    "    mean = moyenne(data) \n",
    "    dev = deviation(data)\n",
    "    for i in range(len(data)):\n",
    "        data1 += [( (data[i][0] - mean ) / dev , data[i][1] ) ]              \n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = normalize(app)\n",
    "val = normalize(val) \n",
    "lab_app = normalize(lab_app)\n",
    "lab_val = normalize(lab_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-2.9525e-08)\n"
     ]
    }
   ],
   "source": [
    "# Afficher la moyenne et la d√©viation standard de chaque dataset\n",
    "print(moyenne(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(deviation(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-2.9525e-08)\n"
     ]
    }
   ],
   "source": [
    "print(moyenne(lab_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(deviation(lab_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-2.8302e-08)\n"
     ]
    }
   ],
   "source": [
    "print(moyenne(lab_app))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(deviation(lab_app))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "#### 1.4. Cr√©ation de Pytorch Dataset\n",
    "\n",
    "Notre Pipeline devrait commencer par le chargement des donn√©es dans le NN. Pytorch fournit un ensemble d'outils facilitant et optimisant cette t√¢che. La classe **Dataset** permet de cr√©er, sur la base de nos donn√©es, un dataset personalis√© qui pourra √™tre utilis√© par la suite par la fonction built-in **DataLoader** afin d'alimenter les donn√©es lors de l'entra√Ænement du NN. Dataset fournit un acc√®s uniforme √† nos donn√©es. DataLoader joue le r√¥le d'un data feeder en cr√©ant les batches de donn√©es qui fournissent au r√©seau de neuronne un √©chantillon de donn√©e √† chaque it√©ration.\n",
    "\n",
    "Documentation classe Dataset: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
    "\n",
    "Afin de cr√©er notre Pytorch Dataset personalis√©, nous allons h√©riter de la classe Dataset et red√©finir les fonctions suivantes:\n",
    "*   \\_\\_init__(self, data, targets): constructeur qui assignera les valeurs des attributs data et targets √† l'objet une fois instanci√©\n",
    "*   \\_\\_getitem__(self, idx) : permet de r√©cup√©rer l'item √† l'index idx. Utile pour √©viter de charger la totalit√© du Dataset en m√©moire (s'il est volumineux, on pr√©cise le chemin dans  \\_\\_init__), \n",
    "*   \\_\\_len__(self) : retroune la longeur de notre vecteur target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer Dataset de torch.utils.data\n",
    "from torch.utils.data import Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une classe qui h√©rite de Dataset et red√©finit les m√©thodes comme susmentionn√©\n",
    "class class1(Dataset):\n",
    "    def __init__(self,data,targets):\n",
    "        self.__data = data\n",
    "        self.targets = targets\n",
    "    def __getitem(self,idx):\n",
    "        return self.data[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Nous allons instancier 3 objets (√† partir de votre classe personnalis√©e) pour les donn√©es et targets de l'apprentissage, test et validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les 3 objets en instantiant votre classe\n",
    "obj1 = class1(app,lab_app)\n",
    "obj2 = class1(val,lab_val)\n",
    "obj3 = class1(TrainSet,TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer DataLoader de torch.utils.data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> DataLoader aide pour l'√©chantillonage et l'organisation de nos donn√©es en mini-batches. A chaque it√©ration d'apprentissage (epoch), le DataLoader fait un shuffling des donn√©es avant de les passer au NN. DataLoader prend le dataset et la taille du batch comme argument. On mettra pour l'instant une taille de 64.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une variable pour la taille du batch\n",
    "x = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er les objets DataLoader pour vos datasets d'apprentissage, test et validation en lui donner la taille du batch convenue\n",
    "d_app = DataLoader(app , batch_size=x , shuffle=True )\n",
    "d_val = DataLoader(val , batch_size=x , shuffle=True )\n",
    "d_lab_val = DataLoader(lab_val , batch_size=x , shuffle=True )\n",
    "d_lab_app = DataLoader(lab_app , batch_size=x , shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "# Quelle est la taille de vos objets dataLoader? Comment expliquer ces r√©sultats?\n",
    "len(d_app)\n",
    "# DataLoader retourne une liste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### 2. Cr√©er le r√©seau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch fournit le module nn qui comprend les classes repr√©sentant les briques de base pour construire un r√©seau de neurones. Il y a maintes mani√®res de cr√©er un NN sur Pytorch, nous allons explorer quelques unes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer le module nn\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Pytorch permet de concat√©ner des classes de notre choix pour la cr√©ation de notre NN, et ce gr√¢ce √† nn.Sequential(). Cette m√©thode prend les donn√©es en input, et se charge de faire passer les outputs interm√©diaires aux modules suivants, puis produit l'output du dernier module.\n",
    "\n",
    "Pour notre NN, nous allons cr√©er une architecture compos√©e de:\n",
    "\n",
    "1. Input Layer: Nos images ont une taille 28 x 28. On va essayer de r√©duire la taille en 64\n",
    "2. Fonction d'activation: ReLu\n",
    "3. Hidden Layer: applique une transformation lin√©aire sur les donn√©es en input, et r√©duit leur dimension en 32\n",
    "4. Fonction d'activation: ReLu\n",
    "5. Output Layer: applique une transformation lin√©aire sur les donn√©es en input, et r√©duit leur dimension en 10 (Nombre de classes)\n",
    "\n",
    "Le choix que nous venons de faire des nombres des neurones dans hidden Layer est al√©atoire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En utilisant Sequential(), cr√©er un mod√®le avec l'architecture susmentionn√©e\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(28*28,64,10),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64,32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32,10)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Conv2d(784, 64, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=32, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "# Afficher le nombre de param√®tres qui seront entra√Æn√©s par le NN cr√©√©. Pourquoi ce chiffre?\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Sequential est une fonction standard de Pytorch. Elle n'offre pas assez de marge de personalisation. C'est pour autant qu'on a tendance √† cr√©er nos propres classe de r√©seaux de neurones, en h√©ritant de la classe nn.Module et en red√©finissant ses m√©thodes.\n",
    "\n",
    "La cr√©ation de notre Classe r√©seau de neurones se fera en h√©ritant de nn.Module et en red√©finissant:\n",
    "1. La m√©thode \\_\\_init__ o√π on initialise les couches √† utiliser (on suivra la m√™me architecture que le premier NN)\n",
    "2. La m√©thode forward o√π on sp√©cifie les connexions entre les couches. Cela permet de pr√©ciser les op√©rations qu'on appliquera sur les donn√©es √† chaque passe (et leur passage de l'input, hidden, √† l'output layer).\n",
    "\n",
    "\n",
    "Dans la fonction forward, on utilisera le module **torch.nn.Functional** qui contient un ensemble de fonctions utiles comme les fonctions d'activation... Vous pouvez voir un exemple de d√©finition d'un NN personalis√© sur la documentation de pytorch: https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer torch.nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsXCHwP7RP0b"
   },
   "outputs": [],
   "source": [
    "# Cr√©er une classe qui h√©rite de nn.Module et red√©finir le constructeur ainsi que la m√©thode forward\n",
    "class class2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(class2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(28*28,64,10)\n",
    "        self.Relu1 = nn.ReLU()\n",
    "        self.Linear1 = nn.Linear(64,32)\n",
    "        self.Relu2 = nn.ReLU()\n",
    "        self.Linear2 = nn.Linear(32,10)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Relu1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Relu1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.Linear2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancier votre NN\n",
    "NN = class2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "class2(\n",
       "  (conv1): Conv2d(784, 64, kernel_size=(10, 10), stride=(1, 1))\n",
       "  (Relu1): ReLU()\n",
       "  (Linear1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (Relu2): ReLU()\n",
       "  (Linear2): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "# V√©rifier que vous avez le m√™me nombre de param√®tres que le premier NN\n",
    "NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### 3. Entra√Æner le r√©seau de neuronne\n",
    "\n",
    "Maintenant que nous avons notre r√©seau de neurones, on devra commencer √† l'entra√Æner. La boucle d'apprentissage est le processus qui permettra √† notre r√©seau de neurones d'apprendre √† partir des donn√©es input et d'ajuster les poids du NN selon les r√©sultats de la fonction Loss. Pour impl√©menter notre boucle d'apprentissage sur Pytorch, nous avons besoin de:\n",
    "\n",
    "1. Pr√©ciser un nombre d'it√©ration pour l'apprentissage (epochs)\n",
    "2. D√©finir notre fonction co√ªt\n",
    "3. Choisir l'optimiseur qui permettra d'optimiser le co√ªt\n",
    "\n",
    "    \n",
    "Notre boucle d'apprentissage se fera selon le nombre des epochs d√©fini, et devra comporter: \n",
    "1. Une boucle sur les donn√©es d'entra√Ænement via DataLoader (minibatch): Alimenter les donn√©es dans le NN, Calculer le co√ªt, et faire la backpropagation. \n",
    "2. Une boucle sur les donn√©es de validation via DataLoader:  Alimenter les donn√©es dans le NN, Calculer le co√ªt, calculer un score de pr√©cision du mod√®le (on va d√©finir un score simple qui indique le nombre de pr√©visions correctes).\n",
    "\n",
    "Une bonne pratique est d'afficher les valeurs de co√ªt et de pr√©cision pour chaque epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir la fonction du co√ªt. On peut choisir CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir une fonction d'optimisation des co√ªt: Adam par exemple. On devra d√©finir un learning rate. On choisira 0.001.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir le nombre d'epochs. Commencer petit. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une boucle sur les epochs:\n",
    "    \n",
    "    # Sp√©cifier qu'on est sur le mode entra√Ænement\n",
    "    \n",
    "    # initialiser notre co√ªt d'apprentissage √† 0.0\n",
    "    \n",
    "    # Boucler sur les minibatchs des donn√©es d'enta√Ænement (les donn√©es et leurs targets):\n",
    "        # le vecteur des labels pr√©dites par le mod√®le est le r√©sultat de l'application du mod√®le sur le minibatch en cours. \n",
    "        # Nous aurons besoin d'applatir les donn√©es avant de les donner √† notre NN\n",
    "        \n",
    "        # Calculer le co√ªt en comparant les labels pr√©dits aux targets du minibatch\n",
    "        \n",
    "        # Backpropagation: \n",
    "        # R√©initialiser l'optimiseur\n",
    "        # Faire la backpropagation\n",
    "        # Effectuer un pas d'optimisation\n",
    "\n",
    "        # Mettre √† jour votre co√ªt d'apprentissage en lui ajoutant le co√ªt du data batch\n",
    "    \n",
    "    # A la sortie de la boucle de l'entra√Ænement, on calcule le co√ªt moyen pour toutes les donn√©es training\n",
    "    \n",
    "    # Initiliser le co√ªt de validation √† 0.0\n",
    "    \n",
    "    # Initialiser le nombre de pr√©visions correctes √† 0\n",
    "    \n",
    "    # Sp√©cifier qu'on est sur le mode d'√©valuation\n",
    "    \n",
    "    # Indiquer √† Pytorch qu'on ne va pas faire de Gradient descent (comme on est dans l'√©valuation)\n",
    "        \n",
    "        # Boucler sur les minibatchs des donn√©es de validation (les donn√©es et leurs targets):\n",
    "            \n",
    "            # le vecteur des labels pr√©dites par le mod√®le est le r√©sultat de l'application du mod√®le sur le minibatch en cours. \n",
    "            # Nous aurons besoin d'applatir les donn√©es avant de les donner √† notre NN\n",
    "            \n",
    "            # Calculer le co√ªt en comparant les labels pr√©dits aux targets du minibatch\n",
    "            \n",
    "            # Mettre √† jour votre co√ªt de validation en lui ajoutant le co√ªt du data batch\n",
    "            \n",
    "            # Mettre √† jour le nombre de pr√©vision correctes en y ajoutant le nombre des bonnes pr√©vision sur ce batch\n",
    "            # On y compare le label pr√©dit avec le labels du minibatch. \n",
    "            # Penser √† utiliser argmax pour avoir la pr√©vision finale √† partir du vecteur de pr√©vision\n",
    "\n",
    "        # A la sortie de cette boucle, calculer le co√ªt moyen de validation\n",
    "        \n",
    "        \n",
    "        # Calculer la pr√©cision: la moyenne des pr√©visions correctes sur l'ensemble des observations dans le dataset validation \n",
    "        \n",
    "\n",
    "    # Afficher pour chaque it√©ration le co√ªt d'entra√Ænement, le co√ªt de validation, et la pr√©cision.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### 4. Tester le mod√®le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Tester notre mod√®le revient √† calculer la pr√©cision de la m√™me mani√®re que nous avons fait tout √† l'heure, mais sur les donn√©es test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliser le co√ªt de test √† 0.0\n",
    "    \n",
    "# Initialiser le nombre de pr√©visions correctes √† 0\n",
    "    \n",
    "# Indiquer √† Pytorch qu'on ne va pas faire de Gradient descent (comme on est dans l'√©valuation)\n",
    "        \n",
    "    # Boucler sur les minibatchs des donn√©es de test (les donn√©es et leurs targets):\n",
    "            \n",
    "            # le vecteur des labels pr√©dites par le mod√®le est le r√©sultat de l'application du mod√®le sur le minibatch en cours. \n",
    "            # Nous aurons besoin d'applatir les donn√©es avant de les donner √† notre NN\n",
    "            \n",
    "            # Calculer le co√ªt en comparant les labels pr√©dits aux targets du minibatch\n",
    "            \n",
    "            # Mettre √† jour votre co√ªt de test en lui ajoutant le co√ªt du data batch\n",
    "            \n",
    "            # Mettre √† jour le nombre de pr√©vision correctes en y ajoutant le nombre des bonnes pr√©vision sur ce batch\n",
    "            # On y compare le label pr√©dit avec le labels du minibatch. \n",
    "            # Penser √† utiliser argmax pour avoir la pr√©vision finale √† partir du vecteur de pr√©vision\n",
    "\n",
    "\n",
    "        # A la sortie de cette boucle, calculer le co√ªt moyen de test\n",
    "        \n",
    "        \n",
    "        # Calculer la pr√©cision: la moyenne des pr√©visions correctes sur l'ensemble des observations dans le dataset test\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oLSMwg84dHjf"
   ],
   "name": "Coding AI - Intro to PyTorch - Skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}